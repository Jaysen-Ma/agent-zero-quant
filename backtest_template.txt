import pandas as pd
import seaborn as sns
import backtrader as bt
import joblib
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from catboost import CatBoostClassifier
import arcticdb as adb
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import logging
import sys

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,  # Set to DEBUG for detailed logs during testing
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        # You can add FileHandler here to log to a file
    ]
)

logger = logging.getLogger(__name__)

class DataPreparer:
    def __init__(self, 
                 db_url="lmdb://data/TimeSeriesDB", 
                 library_name='symbol_specific', 
                 symbol='BTCUSD',
                 target_lookahead=10,  # in minutes
                 risk_percentage=0.0003,  # 0.03%
                 reward_percentage=0.0006,  # 0.06%
                 lot_size=0.01):
        """
        Initializes the DataPreparer with database connection and trading parameters.

        Parameters:
        - db_url (str): The ArcticDB URL.
        - library_name (str): The library name within ArcticDB.
        - symbol (str): The trading symbol, e.g., 'EURUSD'.
        - target_lookahead (int): Number of minutes to look ahead for target creation.
        - risk_percentage (float): Risk per trade (e.g., 0.0003 for 0.03%).
        - reward_percentage (float): Reward per trade (e.g., 0.0006 for 0.06%).
        - lot_size (float): The lot size per trade (e.g., 0.01).
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.debug("Initializing DataPreparer with parameters: db_url=%s, library_name=%s, symbol=%s, "
                          "target_lookahead=%s, risk_percentage=%s, reward_percentage=%s, lot_size=%s",
                          db_url, library_name, symbol, target_lookahead, risk_percentage, reward_percentage, lot_size)
        
        self.db_url = db_url
        self.library_name = library_name
        self.symbol = symbol
        self.target_lookahead = target_lookahead
        self.risk_percentage = risk_percentage
        self.reward_percentage = reward_percentage
        self.lot_size = lot_size
        
        # Initialize placeholders
        self.data: pd.DataFrame = pd.DataFrame()
        self.features = []
        self.X_train = self.X_test = self.y_train = self.y_test = None
        self.scaler = None
        self.model = None

    def load_data(self):
        """
        Connects to ArcticDB and retrieves the data for the specified symbol.
        """
        self.logger.info("Starting to load data from ArcticDB.")
        try:
            # Connect to ArcticDB
            arctic = adb.Arctic(self.db_url)
            lib = arctic.get_library(self.library_name)
            self.logger.debug("Connected to ArcticDB at %s, library: %s", self.db_url, self.library_name)
            
            # Read data
            self.data = lib.read(self.symbol).data.copy()
            self.logger.debug("Data columns: %s", self.data.columns.tolist())
            self.data.sort_index(inplace=True)
            self.logger.info("Data for %s loaded successfully with %d records.", self.symbol, len(self.data))
        except Exception as e:
            self.logger.error("Error loading data: %s", e)
            raise

    def create_target(self):
        """
        Creates the target variable based on the specified risk/reward ratio and lookahead period.
        The target has three classes:
        - 0: Bullish (price increases by at least reward_percentage within lookahead)
        - 1: Bearish (price decreases by at least reward_percentage within lookahead)
        - 2: Neutral (price does not move beyond the thresholds)
        """
        self.logger.info("Creating target variable.")
        try:
            # Create a copy of the original data to avoid modifying it directly
            df = self.data.copy()
            self.logger.debug("Original data shape: %s", df.shape)
            
            # Calculate the highest and lowest future prices within the lookahead period
            df['future_high'] = df['close'].shift(-self.target_lookahead).rolling(window=self.target_lookahead).max()
            df['future_low'] = df['close'].shift(-self.target_lookahead).rolling(window=self.target_lookahead).min()
            self.logger.debug("Calculated future_high and future_low.")
            
            # Calculate the percentage change from the current close price to the future high and low prices
            df['pct_change_high'] = (df['future_high'] - df['close']) / df['close']
            df['pct_change_low'] = (df['future_low'] - df['close']) / df['close']
            self.logger.debug("Calculated pct_change_high and pct_change_low.")
            
            # Define the target variable based on the percentage changes
            # 0: Bullish (future high >= reward percentage)
            # 1: Bearish (future low <= -reward percentage)
            # 2: Neutral (neither condition met)
            conditions = [
                df['pct_change_high'] >= self.reward_percentage,
                np.abs(df['pct_change_low']) >= self.reward_percentage
            ]
            choices = [0, 1]  # 0: Bullish, 1: Bearish
            df['target'] = np.select(conditions, choices, default=2)
            self.logger.debug("Target variable created with conditions applied.")
            
            # Drop auxiliary columns
            df.drop(['future_high', 'future_low', 'pct_change_high', 'pct_change_low'], axis=1, inplace=True)
            self.logger.debug("Dropped auxiliary columns.")
            
            # Drop rows with NaN values resulting from shifting
            initial_shape = df.shape
            df.dropna(inplace=True)
            self.logger.debug("Dropped NaN rows. Shape before: %s, after: %s", initial_shape, df.shape)
            
            self.data = df
            self.logger.info("Target variable created with classes: 0 (Bullish), 1 (Bearish), 2 (Neutral).")
        except Exception as e:
            self.logger.error("Error creating target variable: %s", e)
            raise

    def select_features(self, feature_list=None):
        """
        Selects the feature columns for model training.

        Parameters:
        - feature_list (list): Optional list of features to include. If None, all technical indicators are used.
        """
        self.logger.info("Selecting features for model training.")
        try:
            if feature_list is None:
                self.features = [
                    'open', 'high', 'low', 'close',
                    'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100',
                    'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100',
                    'RSI_14', 'MACD_12_26_9_MACD_12_26_9',
                    # Add other features as necessary
                ]
                self.logger.debug("Default feature list selected: %s", self.features)
            else:
                self.features = feature_list
                self.logger.debug("Custom feature list selected: %s", self.features)
            
            self.logger.info("Selected %d features for model training.", len(self.features))
        except Exception as e:
            self.logger.error("Error selecting features: %s", e)
            raise

    def split_data(self, test_size=0.2):
        """
        Splits the data into training and testing sets without shuffling to maintain temporal order.

        Parameters:
        - test_size (float): Proportion of the dataset to include in the test split.
        """
        self.logger.info("Splitting data into training and testing sets with test_size=%.2f.", test_size)
        try:
            X = self.data[self.features].dropna()
            y = self.data.loc[X.index, 'target']
            self.logger.debug("Features shape: %s, Target shape: %s", X.shape, y.shape)
            
            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
                X, y, test_size=test_size, shuffle=False)
            
            self.logger.info("Data split into training (%d) and testing (%d) sets.",
                             len(self.X_train), len(self.X_test))
        except Exception as e:
            self.logger.error("Error splitting data: %s", e)
            raise

    def normalize_features(self):
        self.logger.info("Normalizing features using StandardScaler.")
        try:
            self.scaler = StandardScaler()
            
            # Fit on training data
            self.scaler.fit(self.X_train[self.features])
            self.logger.debug("Scaler fitted on training data.")
            
            self.X_train[self.features] = self.scaler.transform(self.X_train[self.features])
            self.logger.debug("Training features transformed.")
            
            # Transform test data using the same scaler
            self.X_test[self.features] = self.scaler.transform(self.X_test[self.features])
            self.logger.debug("Testing features transformed.")
            
            self.logger.info("Features normalized successfully.")
        except Exception as e:
            self.logger.error("Error normalizing features: %s", e)
            raise

    def prepare(self):
        """
        Executes the full data preparation pipeline.
        """
        self.logger.info("Starting data preparation pipeline.")
        try:
            self.load_data()
            self.create_target()
            self.select_features()
            self.split_data()
            self.normalize_features()
            self.logger.info("Data preparation completed successfully.")
        except Exception as e:
            self.logger.error("Error during data preparation: %s", e)
            raise

    def get_data(self):
        """
        Returns the prepared data splits.

        Returns:
        - X_train (DataFrame): Training features.
        - X_test (DataFrame): Testing features.
        - y_train (Series): Training targets.
        - y_test (Series): Testing targets.
        """
        self.logger.debug("Retrieving prepared data splits.")
        return self.X_train, self.X_test, self.y_train, self.y_test

    def save_scalers(self, train_scaler_path='scaler_train.pkl'):
        """
        Saves the fitted scaler to disk for future use.

        Parameters:
        - train_scaler_path (str): File path to save the training scaler.
        """
        self.logger.info("Saving scalers to disk.")
        try:
            joblib.dump(self.scaler, train_scaler_path)
            self.logger.info("Scaler saved to %s.", train_scaler_path)
        except Exception as e:
            self.logger.error("Error saving scalers: %s", e)
            raise

    def train_model(self, iterations=1000, depth=6, learning_rate=0.1, loss_function='MultiClass', l2_leaf_reg=3.0, border_count=254, thread_count=-1):
        """
        Trains a CatBoostClassifier on the prepared training data.

        Parameters:
        - iterations (int): Number of trees.
        - depth (int): Depth of the trees.
        - learning_rate (float): Learning rate.
        - loss_function (str): Loss function to use.
        - l2_leaf_reg (float): L2 regularization term on weights.
        - border_count (int): The number of splits for numerical features.
        - thread_count (int): Number of threads to use. -1 means use all available.
        """
        self.logger.info("Training CatBoost model with iterations=%d, depth=%d, learning_rate=%.2f, loss_function=%s, l2_leaf_reg=%.2f, border_count=%d, thread_count=%d.",
                         iterations, depth, learning_rate, loss_function, l2_leaf_reg, border_count, thread_count)
        try:
            self.model = CatBoostClassifier(
                iterations=iterations,
                depth=depth,
                learning_rate=learning_rate,
                loss_function=loss_function,
                l2_leaf_reg=l2_leaf_reg,
                border_count=border_count,
                thread_count=thread_count,
                verbose=100,
                random_seed=42,
                task_type='GPU',  # Use GPU for acceleration
                devices='0'  # Specify GPU device ID, '0' for first GPU (optional if multiple GPUs)
            )
            
            self.logger.debug("CatBoostClassifier initialized.")
            
            self.model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))
            self.logger.info("CatBoost model trained successfully.")
        except Exception as e:
            self.logger.error("Error training CatBoost model: %s", e)
            raise

    def evaluate_model(self):
        """
        Evaluates the trained model on the testing set and prints classification metrics.
        """
        self.logger.info("Evaluating the trained model.")
        try:
            if self.model is None:
                self.logger.warning("Model is not trained yet.")
                return

            # Predict on the test set
            y_pred = self.model.predict(self.X_test)
            self.logger.debug("Model predictions: %s", y_pred[:10])  # Log first 10 predictions

            # Display the classification report as a DataFrame
            self.logger.info("Classification Report:")
            report = classification_report(self.y_test, y_pred, output_dict=True)
            report_df = pd.DataFrame(report).transpose()
            self.logger.debug("Classification Report DataFrame:\n%s", report_df)

            # You can choose to log the report or handle it as needed
            # For example, save to a file or log specific metrics
            self.logger.info("\n%s", report_df.to_string())

            # Display the confusion matrix using a heatmap
            self.logger.info("Generating confusion matrix.")
            conf_matrix = confusion_matrix(self.y_test, y_pred)
            plt.figure(figsize=(8, 6))
            sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,
                        xticklabels=['Pred 0', 'Pred 1', 'Pred 2'], yticklabels=['True 0', 'True 1', 'True 2'])
            plt.title("Confusion Matrix")
            plt.ylabel("Actual Label")
            plt.xlabel("Predicted Label")
            plt.savefig('confusion_matrix.png')
            self.logger.info("Confusion matrix saved as confusion_matrix.png.")
            plt.close()

            # Optionally, display accuracy and other relevant metrics separately
            accuracy = report.get('accuracy', 0)
            self.logger.info("Model Accuracy: %.2f%%", accuracy * 100)
        except Exception as e:
            self.logger.error("Error evaluating model: %s", e)
            raise

    def save_model(self, model_path='catboost_model.cbm'):
        """
        Saves the trained CatBoost model to disk.

        Parameters:
        - model_path (str): File path to save the model.
        """
        self.logger.info("Saving CatBoost model to %s.", model_path)
        try:
            if self.model is not None:
                self.model.save_model(model_path)
                self.logger.info("Model saved to %s.", model_path)
            else:
                self.logger.warning("No model to save.")
        except Exception as e:
            self.logger.error("Error saving model: %s", e)
            raise

class CatBoostStrategy(bt.Strategy):
    params = (
        ('model_path', 'catboost_model.cbm'),
        ('train_scaler_path', 'scaler_train.pkl'),  # Use the training scaler
        ('lookahead', 10),  # Minutes to look ahead
        ('risk_pct', 0.0003),
        ('reward_pct', 0.0006),
        ('lot_size', 0.01),
        ('printlog', True),
        ('order_interval', 4),  # Interval in minutes between orders
        ('stop_loss', 0.0003),    # 0.1% stop-loss
        ('take_profit', 0.0006),  # 0.2% take-profit
    )

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.model = CatBoostClassifier()
        self.model.load_model(self.params.model_path)
        self.scaler = joblib.load(self.params.train_scaler_path)
        self.history = []
        self.orders = []  # To track multiple orders
        self.last_order_time = None  # To track the last order time
        self.active_trade = None  # Track active trade

    def notify_order(self, order):
        if order.status in [order.Submitted, order.Accepted]:
            return

        if order.status in [order.Completed]:
            if order.isbuy():
                self.logger.info("BUY order completed at price: %.5f", order.executed.price)
            elif order.issell():
                self.logger.info("SELL order completed at price: %.5f", order.executed.price)
        elif order.status in [order.Canceled, order.Margin, order.Rejected]:
            self.logger.warning("Order canceled/margin/rejected")

        # Remove the order from the tracking list
        if order in self.orders:
            self.orders.remove(order)

    def notify_order(self, order):
        if order.status in [order.Submitted, order.Accepted]:
            return

        if order.status in [order.Completed]:
            if order.isbuy():
                self.logger.info("BUY order completed at price: %.5f", order.executed.price)
            elif order.issell():
                self.logger.info("SELL order completed at price: %.5f", order.executed.price)
        elif order.status in [order.Canceled, order.Margin, order.Rejected]:
            action = 'BUY' if order.isbuy() else 'SELL'
            self.logger.warning(
                f"Order {action} rejected/canceled/margin: "
                f"Type: {order.exectype}, Size: {order.size}, Price: {order.created.price}"
            )

        # Remove the order from the tracking list
        if order in self.orders:
            self.orders.remove(order)

    def log(self, txt, dt=None):
        if self.params.printlog:
            dt = dt or self.datas[0].datetime.date(0)
            self.logger.info('%s %s', dt.isoformat(), txt)

    def next(self):
        current_datetime = self.datas[0].datetime.datetime(0)
        self.logger.info("Current datetime: %s", current_datetime)
        # Append current data to history
        current_data = {
            'open': self.data.open[0],
            'high': self.data.high[0],
            'low': self.data.low[0],
            'close': self.data.close[0],
            'SMA_10': self.data.SMA_10[0],
            'SMA_20': self.data.SMA_20[0],
            'SMA_50': self.data.SMA_50[0],
            'SMA_100': self.data.SMA_100[0],
            'EMA_10': self.data.EMA_10[0],
            'EMA_20': self.data.EMA_20[0],
            'EMA_50': self.data.EMA_50[0],
            'EMA_100': self.data.EMA_100[0],
            'RSI_14': self.data.RSI_14[0], 
            'MACD_12_26_9_MACD_12_26_9': self.data.MACD_12_26_9_MACD_12_26_9[0],
        }
        self.history.append(current_data)
        self.logger.debug("Appended new data to history: %s", current_data)

        
        # Create a DataFrame from history
        df = pd.DataFrame(self.history)
        
        # Feature Selection
        features = [
            'open', 'high', 'low', 'close',
            'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100',
            'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100',
            'RSI_14', 'MACD_12_26_9_MACD_12_26_9'
        ]
    
        
        # Extract the latest feature set as a DataFrame
        latest_features = df[features].iloc[[-1]]
        
        # Check for NaN values in features
        if latest_features.isnull().any().any():
            self.logger.warning("NaN values found in features. Skipping prediction.")
            return
        
        # Normalize features using the training scaler
        try:
            scaled_features = self.scaler.transform(latest_features)
        except Exception as e:
            self.logger.error(f"Error during feature scaling: {e}")
            return
        
        # Predict the class
        try:
            prediction = self.model.predict(scaled_features)[0]
            self.logger.info("Prediction made: %d", prediction)
        except Exception as e:
            self.logger.error(f"Error during prediction: {e}")
            return
        
        # Implement trading logic based on prediction
        # Check if enough time has passed since the last order
        if self.last_order_time is None or (current_datetime - self.last_order_time).total_seconds() >= self.params.order_interval * 60:
            if prediction == 0:  # Bullish
                self.logger.debug("Bullish prediction detected.")
                # Create a Market Buy Order without specifying price
                buy_order = self.buy(size=self.params.lot_size, exectype=bt.Order.Market)
                self.current_order = buy_order  # Track the current order for stop loss
                self.last_order_time = current_datetime
                self.log('BUY ORDER CREATED')
                self.logger.info("BUY order created at market price.")
                
            elif prediction == 1:  # Bearish
                self.logger.debug("Bearish prediction detected.")
                # Create a Market Sell Order without specifying price
                sell_order = self.sell(size=self.params.lot_size, exectype=bt.Order.Market)
                self.current_order = sell_order  # Track the current order for stop loss
                self.last_order_time = current_datetime
                self.log('SELL ORDER CREATED')
                self.logger.info("SELL order created at market price.")
            else:
                # Neutral prediction; do nothing
                self.logger.debug("Neutral prediction detected. No action taken.")
        else:
            minutes_since_last_order = (current_datetime - self.last_order_time).total_seconds() / 60
            self.logger.debug(
                "Order interval not reached. Time since last order: %.2f minutes.", 
                minutes_since_last_order
            )

class CustomPandasData(bt.feeds.PandasData):
    # Add additional lines (columns from your DataFrame)
    lines = (
        'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100', 
        'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100', 
        'RSI_14', 'MACD_12_26_9_MACD_12_26_9',
    )
    
    # Define the default parameters for the additional columns
    params = (
        ('datetime', None),  # Use the DataFrame's DatetimeIndex
        ('open', 'open'),
        ('high', 'high'),
        ('low', 'low'),
        ('close', 'close'),
        ('volume', 'tick_volume'),  # Map 'volume' to 'tick_volume'
        ('openinterest', None),  # No openinterest column
        ('SMA_10', 'SMA_10'),
        ('SMA_20', 'SMA_20'),
        ('SMA_50', 'SMA_50'),
        ('SMA_100', 'SMA_100'),
        ('EMA_10', 'EMA_10'),
        ('EMA_20', 'EMA_20'),
        ('EMA_50', 'EMA_50'),
        ('EMA_100', 'EMA_100'),
        ('RSI_14', 'RSI_14'),
        ('MACD_12_26_9_MACD_12_26_9', 'MACD_12_26_9_MACD_12_26_9'),
    )

def run_backtest():
    logger.info("Starting backtest process.")
    try:
        # Initialize data preparation
        data_preparer = DataPreparer(
            db_url="lmdb://data/TimeSeriesDB",
            library_name='symbol_specific',
            symbol='BTCUSD',  # Change to 'EURUSD' if needed
            target_lookahead=10,
            risk_percentage=0.0003,  # 0.03%
            reward_percentage=0.0006,  # 0.06%
            lot_size=0.01
        )
        
        # Prepare the data
        data_preparer.prepare()
        
        # Train and evaluate the model
        data_preparer.train_model(
            iterations=500,
            depth=6,
            learning_rate=0.1,
            loss_function='MultiClass'
        )
        data_preparer.evaluate_model()
        
        # Save the model and scalers
        data_preparer.save_model(model_path='catboost_model.cbm')
        data_preparer.save_scalers(train_scaler_path='scaler_train.pkl')
        
        # Initialize Cerebro
        cerebro = bt.Cerebro()
        logger.debug("Initialized Cerebro engine.")
        
        # Add strategy
        cerebro.addstrategy(CatBoostStrategy, 
                            model_path='catboost_model.cbm',
                            train_scaler_path='scaler_train.pkl',
                            lookahead=10,
                            risk_pct=0.0004,
                            reward_pct=0.00075,
                            lot_size=1,
                            printlog=True)
        logger.debug("Added CatBoostStrategy to Cerebro.")
        
        # Prepare the data feed for Backtrader
        # Load data from ArcticDB again or use the already loaded data
        arctic = adb.Arctic("lmdb://data/TimeSeriesDB")
        lib = arctic.get_library('symbol_specific')
        raw_data = lib.read('BTCUSD').data.copy()  # Change symbol if needed
        logger.debug("Loaded raw data for backtesting.")
        
        # Verify the index
        logger.info("Verifying DataFrame Index.")
        logger.debug("DataFrame Index: %s", raw_data.index)
        logger.debug("Index Type: %s", type(raw_data.index))
        
        # Identify the last date
        last_datetime = raw_data.index.max()
        last_date = last_datetime.date()
        logger.info("Last date in dataset: %s", last_date)
        
        # Define the start and end datetime for the last date
        end_datetime = pd.Timestamp(last_date)
        start_datetime = end_datetime - pd.Timedelta(hours=2) - pd.Timedelta(seconds=1)
        
        # Filter the DataFrame for the last date
        filtered_data = raw_data.loc[start_datetime:end_datetime]
        logger.info("Filtered data for backtest: %s to %s", start_datetime, end_datetime)
        logger.info("Filtered data shape: %s", filtered_data.shape)

        # Optional: Remove duplicates
        if filtered_data.index.duplicated().any():
            logger.warning("Duplicate datetime indices found. Removing duplicates.")
            filtered_data = filtered_data[~filtered_data.index.duplicated(keep='first')]
            logger.info("Duplicates removed. New data shape: %s", filtered_data.shape)
        
        # Handle missing values if necessary
        if filtered_data.isnull().any().any():
            logger.warning("Missing values found in feature columns. Handling them by dropping.")
            filtered_data.dropna(inplace=True)
            logger.info("Missing values dropped. New data shape: %s", filtered_data.shape)
        
        required_columns = [
            'open', 'high', 'low', 'close',
            'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100',
            'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100',
            'RSI_14', 'MACD_12_26_9_MACD_12_26_9'
            ]
        missing_columns = [col for col in required_columns if col not in filtered_data.columns]
        if missing_columns:
            logger.error(f"Missing columns in data: {missing_columns}")
            raise ValueError(f"Missing columns in data: {missing_columns}")

        # Clean volume data
        if not np.isfinite(filtered_data['tick_volume']).all():
            logger.error("Volume data contains NaN or Inf values.")
            filtered_data['tick_volume'].replace([np.inf, -np.inf], np.nan, inplace=True)
            filtered_data['tick_volume'].fillna(0, inplace=True)  # Example handling
            logger.info("Volume data cleaned.")

        # Convert to Backtrader-compatible format with fromdate and todate
        data_bt = CustomPandasData(
            dataname=filtered_data,
            open='open',
            high='high',
            low='low',
            close='close',
            volume='tick_volume',  # Map 'volume' to 'tick_volume'
            SMA_10='SMA_10',  # Use the column name as the value
            SMA_20='SMA_20',
            SMA_50='SMA_50',
            SMA_100='SMA_100',
            EMA_10='EMA_10',
            EMA_20='EMA_20',
            EMA_50='EMA_50',
            EMA_100='EMA_100',
            RSI_14='RSI_14',
            MACD_12_26_9_MACD_12_26_9='MACD_12_26_9_MACD_12_26_9',
            openinterest=None,
            fromdate=start_datetime,  # Set fromdate
            todate=end_datetime        # Set todate
        )
        logger.debug("Backtrader data feed created.")
        
        cerebro.adddata(data_bt)
        logger.debug("Data feed added to Cerebro.")
        
        # Set initial cash
        cerebro.broker.setcash(100000.0)
        logger.info("Initial portfolio value set to $100,000.00.")
        
        # Set commission (assuming spread is handled in strategy)
        cerebro.broker.setcommission(commission=0.0002)
        logger.debug("Commission set to 0.0002.")
        
        # Print starting portfolio value
        starting_value = cerebro.broker.getvalue()
        logger.info('Starting Portfolio Value: $%.2f', starting_value)
        
        # Run the backtest
        logger.info("Running the backtest.")
        cerebro.run()
        logger.info("Backtest completed.")
        
        # Print final portfolio value
        final_value = cerebro.broker.getvalue()
        logger.info('Final Portfolio Value: $%.2f', final_value)
        
        # After filtering and cleaning data
        if not np.isfinite(filtered_data).all().all():
            logger.error("Data contains NaN or Inf values. Cleaning data before plotting.")
            filtered_data.replace([np.inf, -np.inf], np.nan, inplace=True)
            filtered_data.dropna(inplace=True)
            logger.info("Data cleaned. New data shape: %s", filtered_data.shape)
        else:
            logger.info("Data contains only finite values.")

        # Plot the results
        logger.info("Plotting and saving backtest results.")
        fig = cerebro.plot()[0][0]
        fig.savefig('backtest_results.png')
        logger.info("Backtest results saved as 'backtest_results.png'.")
    except Exception as e:
        logger.error("An error occurred during the backtest: %s", e)
        raise

if __name__ == "__main__":
    run_backtest()
